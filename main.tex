\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\usepackage[margin=0.25in]{geometry}
\usepackage{multicol}
\usepackage{xcolor}
\linespread{1.2}

\usepackage{amsmath}%for multiple integrals
\usepackage{amssymb}%for math symbols like R (reals)
\usepackage{esint}%for closed path integrals
\usepackage{mathtools}% for curly weird arrows

%\usepackage{parskip}% stops indentation
\setlength{\columnseprule}{1pt} % sets with of column seperator line
\def\columnseprulecolor{\color{lightgray}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbb{I}}
\renewcommand{\P}{\mathbb{P}}

\newcommand{\F}{\mathcal{F}}

\newcommand{\La}[1]{\mathcal{L}\left\{#1\right\}}
\newcommand{\Linv}[1]{\mathcal{L}^{-1}\left\{#1\right\}}
\newcommand{\sep}{\vspace{-8pt}\hline\vspace{4pt}} % used for separating sections with hlines
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\iff}{\Leftrightarrow}
\renewcommand{\implies}{\Rightarrow}
\newcommand{\del}{\partial}

\usepackage{bm}
% makes vectors bold instead of have arrow above
\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\g}{\vec{g}}
\newcommand{\h}{\vec{h}}
\newcommand{\e}{\vec{e}}
\renewcommand{\c}{\vec{c}}
\renewcommand{\a}{\vec{a}}
\renewcommand{\u}{\vec{u}}
\renewcommand{\v}{\vec{v}}

\newcommand{\Mat}{$\text{Mat}$}
\newcommand{\Maps}{$\text{Maps}$}
\newcommand{\defeq}{:=}
\newcommand{\pr}{\text{pr}}
\newcommand{\bul}{\bullet}

\newcommand{\norm}[1]{\Vert #1 \Vert}

\newcommand{\inj}{\xhookrightarrow{}}

%\newcommand{\gs}{\boxed{\begin{matrix}\text{general} \\ \text{sol}\end{matrix}}

\setlength\parindent{0cm}

%changes section heading font size
\usepackage{sectsty}
\sectionfont{\fontsize{12}{12}\selectfont}
\subsectionfont{\fontsize{10}{10}\selectfont}

%change space between sections
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{0pt}{0pt}

\raggedcolumns

\newcommand{\GS}[1]{{
    \left|\begin{smallmatrix}
        \text{General} \\
        \text{Solution}
    \end{smallmatrix}:#1\right. }}


\renewcommand{\SS}[1]{
\left|\begin{smallmatrix}
    \text{Specific} \\
    \text{Solution}
\end{smallmatrix}:#1\right.}

\begin{document}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt} 

\begin{multicols*}{2}

\section{Preliminaries}
For population $N(t)$, and max supported pop $N^*$. Let $n = \frac{N}{N^*}$.\\
\textbf{Logistic model:} \[\frac{dn}{dt} = rn(1-n) \GS{ n(t) = \frac{ce^{rt}}{1 + ce^{rt}}}.\]\\
\textbf{Logistic model (with predation):} $\frac{dn}{dt} = rn(1 - \frac{n}{s}) - \frac{n^2}{1 - n^2}$
\textbf{Lotka-Volterra predator-prey model:} $n =$ prey population, $p =$ predator population:\\
\[\frac{dn}{dt} = rn(1 - p), \quad \frac{dp}{dt} = p(1 - n)\]\\
\textbf{Converting non-autonomous systems to autonomous:}\\
For non-autonomous system of $d$ equations:\\
$\frac{dy_1}{dt} = f_1(t, y_1, ... y_d)$, \\
$\frac{dy_2}{dt} = f_2(t, y_1, ... y_d)$, \\
$\vdots$\\
$\frac{dy_d}{dt} = f_d(t, y_1, ... y_d)$. \\
We can make it into an autonomous one by introducing the variable $y_{d+1} \equiv t$ in place of $t$, (thus creating an autonomous system of $d + 1$ variables) and introduce new indep var $s$ such that $ds/dt = 1$:\\
$\frac{dy_1}{ds} = f_1(t, y_1, ... y_{d+1})$, \\
$\frac{dy_2}{ds} = f_2(t, y_1, ... y_{d+1})$, \\
$\vdots$\\
$\frac{dy_d}{ds} = f_d(t, y_1, ... y_{d+1})$, \\
$\frac{dy_{d + 1}}{ds} = 1$. \\
\textbf{Picard's Thm:} let the function $f (\cdot, \cdot)$ be a continuous function of its arguments in a region of the plane containing the rectangle $D = \{(t,y) : t_0 \leq t \leq T, |y - y_0| \leq K\}$, $T,K > 0$. Suppose $\exists L$ (Lipschitz constant) such that $|f(t,u) - f(t,v)| \leq L |u - v|$ whenever $(t,u), (t,v) \in D$. Since $D$ closed $\And$ bounded, $\exists M_f > 0$ such that $M_f = \max\{|f(t,u)|:(t,u) \in D\}$. Assume $M_f(T - t_0) \leq K$. Then there exists a unique continuously differentiable function $t \mapsto y(t)$, defined on $[t_0, T]$, such that $\frac{dy}{dt} = f(t,y), y(t_0) = y_0$.

\textit{Note:} $\frac{\del f}{\del y}$ being continuous $\implies$ existence of a Lipschitz constant,\\
We can take $L = \sup_D\left[\frac{\del f}{\del y}\right]$.\\
\textbf{Thm 1.3.2 (Local Existence / Uniqueness of Solutions for ODE Systems):} Suppose $f : \R \to \R^d \times \R^d$ is continuous and has continuous partial derivatives w.r.t. all components of the dependant var $y$ in a neighbourhood of the point $(t_0, y_0)$. Then there is an interval $I = (t_0 - \delta, t_0 + \delta)$ containing a unique function $y$ (continuously differentiable on $I$) that satisfies $\frac{dy}{dt} = f(t,y), y(t_0)  = y_0$.\\


\section{Euler's Method and Taylor Series Methods}
\textbf{Euler's Method:} for finding approximate solutions to $\frac{dy}{dt} = f(t,y)$ in time interval $t \in [a,b]$. \\ Let $t_m = a + mh$, $h \in \R$ small. Start with $y_0 = y(t_0) = y(a)$ and use iteration function $\boxed{y_{n + 1} = y_n + hf(t_n, y_n)}$. $y_i \approx y(t_i)$.\\
\textbf{Error:} $\boxed{e_n = |y_n - y(t_n)|}$.\\
\textbf{Lemma 2.6.1:} Suppose a given sequence of non-negative numbers $(v_n)$ satisfies $v_{n + 1} \leq Av_n + B$, $A > 1, B > 0$. Then for $n = 0,1,2,...$, $\boxed{v_n \leq A^nv_0 + \frac{A^n - 1}{A - 1}B}$.\\
\textbf{Bound on error:} $\boxed{|e_n| \leq e^{(b-a)L}\frac{M}{2L}h = Dh}$ where $h$ is the timestep, $M$ bounds $|y^{\prime\prime}(t)|$, and $L$ bounds $\frac{\del y}{\del t}$.\\
\textbf{Thm 2.6.1} Consider the IVP $\frac{dy}{dt} = f(t,y), y(a) = y_0$. Suppose $\exists$ unique, twice-differentiable function $f$, continuous everywhere with continuous, bounded partial derivative $\left|\frac{\del f}{\del y}\right| < L$ with $L > 0$.Then for $n = 0,1,...,N$, and some $D > 0$, the solution $y_n$ given by Euler's method at $t_n$ satisfies $\boxed{e_n = |y_n - y(t_n)| \leq Dh}$ where $h = (b-a)/N, t_n = a + hn$.\\
\textbf{Big O (Landau) notation:} If method is $O(h^p)$ then the error decays at least as quickly as $h^p$ (for small $h$).\\
$\boxed{(\exists \: h_0, C > 0):( \forall \: 0 < h < h_0), |z|\leq Ch^p \implies z = O(h^p) }$\\
Say $z$ is of order $p$.\\
\textbf{The Flow Map:} Consider IVP $\frac{dy}{dt} = f(t,y), y(a) = y_0$ with unique solution in $t \in [a,b]$. Starting at arbitrary $t_0 \in [a,b], y_0$ we may see where $y(t)$ ends up, denoted $y(t;t_0, y_0)$. \\
Fix $t_0$ and look at the \textbf{flow map} $\boxed{\Phi_{t_0, h}(y_0) = y(t_0 + h;t_0, y_0)}$.
(\textit{actually a family of maps parameterised by $h$}).\\
\textbf{Numerical methods approximate flow maps:} Euler's method approximates flow map with $\hat{\Phi}_{t, h}(y) = y + hf(t,y)$.\\
\textbf{One-step methods:} approximate the solution through the iteration of an approximated flow map.\\
\textbf{Constructing Taylor series methods:} \\
Start with Taylor series:\\
$y(t_0 + h) = y(t_0) + y^\prime(t_0)h + \frac{1}{2}y^{\prime\prime}(t_0)h^2 + \frac{1}{6}y^{\prime\prime\prime}(t_0)h^3 + ...$. \\
Also $y^\prime(t) = f(t, y) \implies$ \\
$y^{\prime\prime} = \frac{d}{dt}f(t,y) = \frac{\del}{\del t} f(t,y)\frac{dt}{dt} + \frac{\del}{\del y} f(t,y)\frac{dy}{dt}$\\
$ = \frac{\del}{\del t} f(t,y) + \frac{\del}{\del y} f(t,y)y^\prime = f_t + f_y f$. (By chain rule)\\
$\Phi_{t,h}(y) = y + hf(t,y) + \frac{1}{2}h^2(f_t(t,y) + f_y(t,y)f(t,y)) + \frac{1}{6}y^{\prime\prime\prime}h^3 + ...$ \\
Which we can truncate to get the 2nd order Taylor series method\\
$\boxed{\hat{\Phi}_{t,h}(y) = y + hf(t,y) + \frac{1}{2}h^2(f_t(t,y) + f_y(t,y)f(t,y))}$.\\


\section{Convergence of One-Step Methods}

\textbf{Def 3.1.2 (Convergence):} A method is said to be \textbf{convergent} iff for any $T$, \[\lim_{\begin{smallmatrix}
    h \to 0 \\ h = T/N
\end{smallmatrix}} \max_{n = 0,1,...,N}{\Vert e_n \Vert} = 0.\]

\textbf{Def 3.2.1 (Local Error):} The \textbf{local error} of a one-step method is the difference between the flow map $\Phi_h$ and it's descrete approximation $\Psi_h$ 
\[ le(y, h) = \Psi_h(y) - \Phi_h(y).\]
It measures how much error is introduced in a single timestep of size $h$.\\

\textbf{Def 3.2.2 (Consistency):} Suppose the local error for our method satisfies 
\[ \Vert le(y, h) \Vert \leq Ch^{p + 1} \]
 where $C$ is a constant that depends on $y(t)$ and it's derivatives, and $p \geq 1$. Then the method is \textbf{consistent} at order $p$.\\

\textbf{Def 3.2.3 (Stability):} Suppose that a method satisfies an $h-$independent Lipschitz condition on $D$ (spatial domain)
 \[ \norm{\Psi_h(u) - \Psi_h(v)} \leq (1 + h\hat L)\norm{u - v} \quad \forall\, u, v \in D. \]
 Then the method is \textbf{stable}. \textit{Note $\hat L$ need not be the same Lipschitz constant as for the vector field.}\\

\textbf{Thm 3.2.1 (Convergence of One-Step Methods):} Given a differential equation and a one-step method $\Psi_h$ which is \textbf{consistent} and \textbf{stable}. Then the method is \textbf{convergent}.\\

\textbf{Interpolating Polynomials:} Given $s$ distinct \textit{abscissa points} $c_0, ... c_s$ and \textit{data points} $g_0, ..., g_s$, there exists a unique interpolating polynomial $P(x) \in \P_{s - 1}$ passing through all points $(c_i, g_i)$.\\

\textbf{Lagrange Polynomials:} For a set of abscissae $c_0, ... c_s$, the Lagrange polynomials $\ell_i, \; i = 1,...,s$ are defined by \[
\ell_i(x) = \prod_{\begin{smallmatrix}i = 1 \\ i \neq j\end{smallmatrix}} \frac{x - c_j}{c_i - c_j}.
\]
The Lagrange polynomial $\ell_i$ is the interpolating polynomial through the data $g_j = \begin{cases*} 1 \text{ if } j = i \\ 0 \text{ if } j \neq i \end{cases*}$. $\{\ell_i\}$ form a basis for $\P_{s - 1}$, and any polynomial $Q(x)$ has the simple form \[
Q(x) = \sum_{i=1}^s Q(c_i) \ell_i(x) =  \sum_{i=1}^s g_i \ell_i(x) .   
\]

\textbf{Numerical Quadrature:} Given a smooth function $g(x) : \R \to \R$, and $s$ \textbf{quadrature points} $0 \leq c_1 < ... < c_s \leq 1$ we can estimate $\int_0^1 g(x) \,dx$ by integrating the corresponding interpolating polynomial $P(x) \in \P_{s - 1}$. Define the weights \[
b_i = \int_0^1 \ell_i(x)\,dx.    
\]
Then our approximate integral is
\[
\int_0^1 g(x)\,dx \approx \int P(x)\,dx = \sum_{i=1}^s g(c_i) \int_0^1\ell_i(x)\,dx = \sum_{i=1}^s b_i g(c_i)
\]
Therefore for interval $[t_0, t_0 + h]$ we have
\[
    \int_{t_0}^{t_0 + h} g(x)\,dx \approx \int_{t_0}^{t_0 + h} P(x) \, dx = \sum_{i=1}^s b_i g(t_0 + hc_i).
\]
A quadrature rule has \textbf{order} $p$ if it integrates any polynomial $\in \P_{p-1}$ exactly. We always have $p \geq s$, and for optimal choice of $c_i$ we have $p = 2s$.

\textbf{One-Step Collocation:} Given an ODE we wish to construct the \textbf{collocation polynomial} $u(t) \in \R^d$ that satisfies 
\begin{equation*}
    u(t_0) = y_0\\
    u^\prime(t_0 + c_ih) = f(u(t_0 + c_i h)).
\end{equation*}
In particular, it agrees with our solut at $t_0$, and it's derivative matches that of the solution at each $c_1,...,c_s$. We can use such a polynomial to approximate a numerical solution to our ODE by decomposing it's derivative $u^\prime$ into Lagrange polynomial components, and then integrating over $[t_0, t_0 + h]$ to get $u(t_0 + h) = u(t_1)$.\\
Let $F_i = u^\prime(t_0 + hc_i)$ be the value of the derivative of the polynomial at node $c_i$. Then 
\begin{align*}
    & F_i = f(y_0 + h\sum_{j = 1}^s a_{ij}F_j), \; \text{(A)}\\
    & y_{n+1} = y_n + h\sum_{i = 1}^s b_i F_i. \; \text{(B)}
\end{align*} 
where 
\begin{align*}
    & a_{ij} = \int_0^{c_i}\ell_j(x)\,dx,\\
    & b_i = \int_0^1\ell_i(x)\,dx.
\end{align*}
First solve the sd-dimensional system of non-linear equations given by (A), and plug into (B).

\textbf{Rem 3.6.1 (Continuous Approximations):} Collocation provides a continuous approximation $u(t)$ of the solution $y(t)$ on each interval $[t_n, t_{n+1}]$.

\textbf{Rem 3.6.2 (Optimal Node Placement):} For optimal order of accuracy, use \textbf{Gauss-Legendre} collocation methods. This means placing nodes at roots of shifted Legendre polynomials. For $s = 1, 2, 3$, the optimal nodes are \begin{align*}
    c_1 = \frac{1}{2}, \quad &p = 2, \\
    c_1 = \frac{1}{2} - \frac{\sqrt{3}}{6},\; c_2 = \frac{1}{2} + \frac{\sqrt{3}}{6}, \quad &p = 4, \\
    c_1 = \frac{1}{2} - \frac{\sqrt{15}}{10},\; c_2 = \frac{1}{2} ,\; c_3 =\frac{1}{2} + \frac{\sqrt{15}}{10}, \quad &p = 6.
\end{align*}\\

\textbf{Runge-Kutta Methods (Autonymous Case):} Generalisation of collocation methods, since they don't have coefficients that rely on integrals of Lagrange polynomials, they can have any coefficients. A \textbf{Runge-Kutta} method is any method of form 
\begin{align*}
    Y_i &= y_n + h \sum_{j = 1}^s a_{ij}f(Y_j), \quad i = 1,...,s,\\
    y_{n+1} &= y_n + h\sum_{i = 1}^s b_i f(Y_i).
\end{align*}
where $s$ is the \textbf{number of stages}, $b_i$ are the \textbf{weights}, and $a_{ij}$ are the \textbf{internal coefficients}. Such a method generates the discrete flow-map
\[
\Psi_h(y) = y + h \sum_{i = 1}^s b_i f(Y_i(y, h)).
\]\\
\textbf{Butcher Tables:} Store coefficients of Runge-Kutta methods in form 
\[
\begin{tabular}{ c | c }
    c & A \\ 
    \hline
     & $b^T$ \\  
   \end{tabular}
\] where $c = (c_i), \; b = (b_j), \; A = (a_{ij})$. If $A$ is lower triangular (with $0$s on diag) then the method is \textbf{explicit}, otherwise \textbf{implicit}.

\textbf{Butcher Table Examples:} \\
Euler Method (Explicit):
\[
\begin{tabular}{ c | c }
    0 & 0 \\ 
    \hline
     & 1 \\  
   \end{tabular}.
\] Trapezoidal (Implicit):
\[
\begin{tabular}{ c | c c }
    0 & \\ 
    1 & $\frac{1}{2}$ & $\frac{1}{2}$ \\
    \hline
     & $\frac{1}{2}$ & $\frac{1}{2}$ \\  
   \end{tabular}.
\]

\textbf{Order Conditions for RK Methods:}\\
Consider an RK method with $b = (b_j), \; A = (a_{ij})$. For the order to be $p$, the following conditions must be satisfied (as well as any conditions for it to be $<p$):
\begin{align*}
    p = 1 &\implies \sum_{i=1}^s b_i = 1, \\
    p = 2 &\implies \sum_{i=1}^s b_ic_i = \frac{1}{2}, \quad \small\text{That is } b^Tc = \frac{1}{2}\normalsize,\\
    p = 3 &\implies \sum_{i=1}^s b_ic_i^2 = \frac{1}{3} \;\text{And}\; \sum_{i=1}^s\sum_{j=1}^s b_ia_{ij}c_j = \frac{1}{6}.
\end{align*}
There is no explicit RK method of order greater than 4. For best order, use Gauss-Legendre methods, which have order $p = 2s$.\\

\textbf{Def 4.1.1 (Fixed Point):} A point $y^*$ is a fixed point of $f(y)$ if $f(y^*) = 0$. Soluion passing through a fixed point will be $y(t) \equiv y^*$, constant in time. Denote set of all fixed points of an ode system as $\F = \{y \in \R^d : \Phi(y) = y\}$\\

\textbf{Def 4.2.1 (Fixed Point of Numerical Method):} Consider a one-step numerical method with described by the map $\Psi_h(y)$. Then a point $y^*$ is a \textbf{fixed point} if $\Psi_h(y^*) = y^*$, and therefore produces the constant in time approximate solution $y_n \equiv y^*$. Denote the set of all fixed points of $\Psi_h$ by $\F_h = \{y \in \R^d : \Psi_h(y) = y\}$.\\

For the Euler method $\F = \F_h$.\\

Fixed points in $\F_h$ that are not fixed points of $\F$ are called \textbf{spurious fixed points}.\\

\textbf{Thm 4.2.1:} For Runge-Kutta methods, $\F_h \supseteq \F$.

Generally spurious fixed points of RK methods move depending on $h$, and usually tend to $\infty$.\\

\textbf{Def 4.3.1 (Stability and Asymptotic Stability):} $y^*$ is:
\textbf{stable} for the given ODE if $\forall \, \varepsilon > 0$ (sufficiently small) $\exists \delta > 0$ such that $\forall \, t > 0$
\[
    \Vert y_0 - y^*\Vert  < \delta \implies \Vert y(t; y_0) - y^* \Vert < \epsilon;
\]
\textbf{asymptotically stable} if it is \textbf{stable} and $\exists \gamma > 0 $ such that for any initial condition such that $\Vert y-0 \Vert < \gamma$ 
\[
    \lim_{t \to \infty} \Vert y(t; y_0) - y^* \Vert = 0;
\]
\textbf{unstable} if it is not \textbf{stable}.\\

\textbf{Thm 4.3.1 (Linearisation Thm):} Consider the equation in $\R^d$ 
\[
\frac{dy}{dt} = By + F(y)
\]
subject to initial condition $y(0) = y_0 \in \R^d$. Assume $B \in \Mat(d, \R)$ has all eigenvalues with negative real parts, and $F(y) \in C^1$ in a neighbourhood of $y = 0 \in \R^d$, with $F(0) = 0 \in \R^d$ and $F^\prime(0) = 0 \in \R^{d \times d}$, where $F^\prime(y)$ is the jacobian of $F$. Then $y = 0 \in \R^d$ is an \textbf{asymptotically stable} critical point. If $B$ has any eigenvalues with positive real part, then $y = 0$ is \textbf{unstable}.\\

\textbf{Thm 4.3.2 (Linearisation Thm II):} Suppose that our derivative $f \in C^2$ has a fixed point $y^*$. If the eigenvalues of 
\[
J = f^\prime(y)    
\]
lie strictly in the left half plane of $\C$, then $y^*$ is \textbf{asymptotically stable}. If J has any eigenvalues in the right half plane of $\C$, then $y^*$ is \textbf{unstable}.\\

\textbf{Def 4.4.1 (Stability and Asymptotic Stability of Maps):}\\
Consider a general map $\Psi : \R \to R$ and fixed point $y^*$ of $\Psi$ such that $\Psi(y^*) = y^*$. Define $y^n(y_0)$ to be $n$ applications of $\Psi$ to $y_0$, so $y^2(y_0) = \Psi(\Psi(y_0))$. We say that $y^*$ is:\\
\textbf{stable} for the given ODE if $\forall \, \varepsilon > 0$ (sufficiently small) $\exists \delta > 0$ such that $\forall \, t > 0$
\[
    \Vert y_0 - y^*\Vert  < \delta \implies \Vert y^n(y_0) - y^* \Vert < \epsilon;
\]
\textbf{asymptotically stable} if it is \textbf{stable} and $\exists \gamma > 0 $ such that for any initial condition such that $\Vert y-0 \Vert < \gamma$ 
\[
    \lim_{t \to \infty} \Vert y^n(y_0) - y^* \Vert = 0;
\]
\textbf{unstable} if it is not \textbf{stable}.\\

\textbf{spectral radius:} Let $K$ be a matrix. Then $\rho(K)$ denotes the \textbf{spectral radius} of $K$, the radius of the smallest circle centered at the origin enclosing all eigenvalues of $K$.

\textbf{Thm 4.4.1 (Spectral Radius and Stability):}\\
Let $z_n = \Vert K^ny_0 \Vert$.\\
Then $z_n \to 0$ as $n \to \infty$ for all $y_0$, iff $\rho(K) < 1$.\\
Moeover, $z_n \to \infty$ for some $y_0$ iff $\rho(K) > 1$.\\
Finally, if $\rho(K) = 1$ then $z_n$ remains bounded as $n \to \infty$.\\

\textbf{Thm 4.4.2 (Stability and Asymptotic Stability of Iteration Maps):} \\
Let $\Psi$ be a smooth $(C^2)$ map.\\
Then the fixed point $y^*$ is \textbf{asymptotically stable} for the iteration $y_{n+1} = \Psi(y_n)$ if 
\[
    \rho(\Psi^\prime(y^*)) < 1.
\] 
The fixed point $y^*$ is \textbf{unstable} if $\rho(\Psi^\prime(y^*)) > 1$.\\
The marginal case $\rho(\Psi^\prime(y^*)) = 1$ is delicate and must be considered on a case-by-case basis.\\

\textbf{Stability Function:} Consider an RK method with $y_{n+1} = R(h\lambda)y_n$ (for scalar ODEs). Then $R(h\lambda)$ is a rational function, and if the method is \textbf{explicit} then $R(h\lambda)$ is a polynomial. Call $R(h\lambda)$ the \textbf{stability function} of our method.

\textbf{Matrix Representation of RK Method (Scalar):}\\
 An RK method for a scalar ODE $y^\prime = \mu y$ with internal stages 
\[
Y_i = y_n + \mu\sum_{j=1}^s a_{ij}Y_j, \quad i = 1,2,...,s    
\]
can be written in matrix form as
\begin{align*}
    Y &= y_n \mathbf{1} + \mu A Y \\
    Y &= y_n(I - \mu A)^{-1}\mathbf{1}     
\end{align*}
where $\mathbf{1} = (1,...,1)^T \in \R^s$. Then
\begin{align*}
    y_{n+1} &= y_n + \mu \sum_{j=1}^s b_j Y_j\\
    y_{n+1} &= y_n + \mu b^T Y 
\end{align*}
and so 
\[
y_{n+1} = R(\mu)y_n, \quad R(\mu) = 1 + \mu b^T(I - \mu A)^{-1}\mathbf{1}.  
\]
\textbf{Matrix Representation of RK Method (Vector):}\\
Consider an ODE $y^\prime = By$, $y \in \R^d$ and RK method definde by
\begin{align*}
    Y_i     &= y_n + h\sum_{j=1}^s a_{ij}BY_j \\
    y_{n+1} &= y_n + h\sum_{j=1}^s b_iBY_i.
\end{align*}
Expand $y_n, y_{n+1}, Y_i$ in the eigenbasis of $B$, and let $U$ be the matrix with eigenvectors of $B$ as columns. Write $B = U\Lambda U^{-1}$ where $\Lambda$ is diagonal matrix of eigenvalues. Define $z_n, Z_i$ by 
\[
y_n = Uz_n, \quad Y_i = UZ_i, \quad i = 1,...,s,    
\]
and then rewrite our RK method as 
\begin{align*}
    Z_i     &= y_n + h\sum_{j=1}^s a_{ij}\Lambda Z_j \\
    z_{n+1} &= z_n + h\sum_{j=1}^s b_i\Lambda Z_j.
\end{align*}
and so, since $\Lambda$ is diagonal, our system decouples into $d$ independent scalar iterations, which we know how to deal with.\\

\textbf{Thm 4.5.1:} Given the ODE $y^\prime = By$, where $y \in \R^d$, and $B$ has a basis of eigenvectors and the eigenvalues $\lambda_1, ..., \lambda_d$, consider applying a given RK method. The RK method has an (asymptotically) stable fixed point at the orogin when aplied to 
\[
\frac{dy}{dt} = By    
\]
iff the same method has an (asymptotically) stable fixed point at the origin when applied to each of the scalar differential equations 
\[
\frac{dy}{dt} = \lambda_i y. 
\]

\textbf{Cor 4.5.1:} Consider a linear ODE $\frac{dy}{dt} = By$ with diagonalizable matrix $B$. Let an RK method be given with stability fucntion $R$. The origin is stable for this RK method to $\frac{dy}{dt} = By$ (at stepsize h) iff
\[
|R( h\lambda)| < 1    
\]
forall eigenvalues $\lambda \in \sigma(B)$. (The origin is unstable if $|R( h\lambda)| > 1 $ for any eigenvalue $\lambda$).\\

\textbf{Stability Region:} The \textbf{stability region} of a numerical method is the set of all points such that $\hat R(\mu) = |R(\mu)| < 1$.\\

RK4 has the stability function
\[
R(\mu) = 1 + \mu + \frac{1}{2}\mu^2 + \frac{1}{6}\mu^3 + \frac{1}{24}\mu^4.
\]

The boundary of the stability region is the set of all values such that $R(\mu) = e^{i\theta}$, that is $R(\mu)$ lies on the unit circle.\\

\textbf{A-Stability:} A numerical method is \textbf{A-stable} if its stability region contains the entire left half-plane. In this case, the method will be stable independent of $h$.



\end{multicols*}

\end{document}
